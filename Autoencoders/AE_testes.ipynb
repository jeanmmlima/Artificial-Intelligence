{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructions LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 9, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 9, 100)            80400     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 9, 1)              101       \n",
      "=================================================================\n",
      "Total params: 121,301\n",
      "Trainable params: 121,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[0.11062296 0.20638636 0.30260974 0.39946944 0.49723363 0.5962577\n",
      " 0.6966715  0.79908675 0.9042391 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# lstm autoencoder recreate sequence\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# define input sequence\n",
    "sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "# reshape input into [samples, timesteps, features]\n",
    "n_in = len(sequence)\n",
    "sequence = sequence.reshape((1, n_in, 1))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_in))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "model.summary()# demonstrate recreation\n",
    "yhat = model.predict(sequence, verbose=0)\n",
    "print(yhat[0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction LSTM Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "[0.16978964 0.29341868 0.40498853 0.5063768  0.6031685  0.69930184\n",
      " 0.7977215  0.90121293]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "# define input sequence\n",
    "seq_in = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "# reshape input into [samples, timesteps, features]\n",
    "n_in = len(seq_in)\n",
    "seq_in = seq_in.reshape((1, n_in, 1))\n",
    "# prepare output sequence\n",
    "seq_out = seq_in[:, 1:, :]\n",
    "n_out = n_in - 1\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_out))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "plot_model(model, show_shapes=True, to_file='predict_lstm_autoencoder.png')\n",
    "# fit model\n",
    "model.fit(seq_in, seq_out, epochs=300, verbose=0)\n",
    "# demonstrate prediction\n",
    "yhat = model.predict(seq_in, verbose=0)\n",
    "print(yhat[0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x137658d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[array([[[0.10243267],\n",
      "        [0.1993747 ],\n",
      "        [0.29919326],\n",
      "        [0.39964893],\n",
      "        [0.50004876],\n",
      "        [0.6002997 ],\n",
      "        [0.7003342 ],\n",
      "        [0.8001249 ],\n",
      "        [0.89969885]]], dtype=float32), array([[[0.16512783],\n",
      "        [0.2889748 ],\n",
      "        [0.402998  ],\n",
      "        [0.50937825],\n",
      "        [0.6102191 ],\n",
      "        [0.7067359 ],\n",
      "        [0.79991096],\n",
      "        [0.8905692 ]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# lstm autoencoder reconstruct and predict sequence\n",
    "from numpy import array\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "# define input sequence\n",
    "seq_in = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "# reshape input into [samples, timesteps, features]\n",
    "n_in = len(seq_in)\n",
    "seq_in = seq_in.reshape((1, n_in, 1))\n",
    "# prepare output sequence\n",
    "seq_out = seq_in[:, 1:, :]\n",
    "n_out = n_in - 1\n",
    "# define encoder\n",
    "visible = Input(shape=(n_in,1))\n",
    "encoder = LSTM(100, activation='relu')(visible)\n",
    "# define reconstruct decoder\n",
    "decoder1 = RepeatVector(n_in)(encoder)\n",
    "decoder1 = LSTM(100, activation='relu', return_sequences=True)(decoder1)\n",
    "decoder1 = TimeDistributed(Dense(1))(decoder1)\n",
    "# define predict decoder\n",
    "decoder2 = RepeatVector(n_out)(encoder)\n",
    "decoder2 = LSTM(100, activation='relu', return_sequences=True)(decoder2)\n",
    "decoder2 = TimeDistributed(Dense(1))(decoder2)\n",
    "# tie it together\n",
    "model = Model(inputs=visible, outputs=[decoder1, decoder2])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "plot_model(model, show_shapes=True, to_file='composite_lstm_autoencoder.png')\n",
    "# fit model\n",
    "model.fit(seq_in, [seq_in,seq_out], epochs=300, verbose=0)\n",
    "# demonstrate prediction\n",
    "yhat = model.predict(seq_in, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,) 9\n",
      "(1, 9, 1)\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_42 (LSTM)               (None, 5)                 140       \n",
      "_________________________________________________________________\n",
      "repeat_vector_21 (RepeatVect (None, 9, 5)              0         \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 9, 5)              220       \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 9, 1)              6         \n",
      "=================================================================\n",
      "Total params: 366\n",
      "Trainable params: 366\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x13ac1bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[0.19158703 0.2660244  0.34132135 0.41904387 0.5007428  0.5881349\n",
      " 0.6832156  0.7884127  0.9068068 ]\n",
      "Model: \"functional_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_42_input (InputLayer)   [(None, 9, 1)]            0         \n",
      "_________________________________________________________________\n",
      "lstm_42 (LSTM)               (None, 5)                 140       \n",
      "=================================================================\n",
      "Total params: 140\n",
      "Trainable params: 140\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 9, 1) for input Tensor(\"lstm_42_input:0\", shape=(None, 9, 1), dtype=float32), but it was called on an input with incompatible shape (None, 1, 1).\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x13b0d2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(9, 5)\n",
      "[[0.         0.         0.00995906 0.         0.06841876]\n",
      " [0.         0.         0.00639378 0.         0.08249621]\n",
      " [0.         0.         0.00261809 0.         0.09694532]\n",
      " [0.         0.         0.         0.         0.11176745]\n",
      " [0.         0.         0.         0.         0.12696384]\n",
      " [0.         0.         0.         0.         0.14253557]\n",
      " [0.         0.         0.         0.00035323 0.15848348]\n",
      " [0.         0.         0.         0.00102564 0.17480832]\n",
      " [0.         0.         0.         0.00173634 0.1915106 ]]\n"
     ]
    }
   ],
   "source": [
    "# lstm autoencoder recreate sequence\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "\n",
    "'''\n",
    "# define input sequence\n",
    "sequence = array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
    "print(sequence.shape)\n",
    "# reshape input into [samples, timesteps, features]\n",
    "n_steps = 1\n",
    "n_inputs = 3\n",
    "n_features = 3\n",
    "n_neurons = 10\n",
    "'''\n",
    "\n",
    "sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "sequence2 = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "\n",
    "print(sequence.shape, len(sequence))\n",
    "n_steps = 9\n",
    "n_inputs = 1\n",
    "n_features = 1\n",
    "n_neurons = 5\n",
    "\n",
    "sequence = sequence.reshape((n_inputs, n_steps, n_features))\n",
    "print(sequence.shape)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model.add(RepeatVector(n_steps))\n",
    "model.add(LSTM(n_neurons, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "model.summary()# demonstrate recreation\n",
    "yhat = model.predict(sequence, verbose=0)\n",
    "print(yhat[0,:,0])\n",
    "# connect the encoder LSTM as the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "model.summary()\n",
    "# get the feature vector for the input sequence\n",
    "yhat = model.predict(sequence2)\n",
    "print(yhat.shape)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[0.68590391 0.54090121 0.88470576]\n",
      " [0.7531608  0.72093559 0.94111915]\n",
      " [0.8011296  0.80879546 0.96230284]]\n",
      "[[0.48111321 0.43956525 0.55430981 0.52468811 0.49056521 0.63360623\n",
      "  0.42089127 0.30110503 0.6965202  0.57480749]\n",
      " [0.53730781 0.34092447 0.62362589 0.63221406 0.49147443 0.81327803\n",
      "  0.31701471 0.13899075 0.79988807 0.6121095 ]\n",
      " [0.59257035 0.25437226 0.68822506 0.72802419 0.49238371 0.91645945\n",
      "  0.22865267 0.05703552 0.87439698 0.64814221]]\n"
     ]
    }
   ],
   "source": [
    "from autoencoder import DenoisingAutoencoder\n",
    "from numpy import array\n",
    "\n",
    "\n",
    "da = DenoisingAutoencoder(n_hidden=10)\n",
    "\n",
    "X = array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
    "\n",
    "n_steps = 9\n",
    "n_inputs = 1\n",
    "n_features = 1\n",
    "n_neurons = 5\n",
    "\n",
    "X = X.reshape((3, 3))\n",
    "print(X.shape)\n",
    "\n",
    "da.fit(X)\n",
    "\n",
    "new_X = da.transform(X)\n",
    "print(new_X)\n",
    "\n",
    "#To change the dimensionality of X (in this case, changed_X will have \"n_hidden\" features)\n",
    "\n",
    "changed_X = da.transform_latent_representation(X)\n",
    "print(changed_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
